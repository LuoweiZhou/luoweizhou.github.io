<!DOCTYPE HTML>
<html lang="en"><head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-58631381-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-58631381-1');
  </script>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Luowei Zhou</title>
  
  <meta name="author" content="Luowei Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ§ </text></svg>">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Luowei Zhou</name>
              </p>
              <p>I am a research scientist at <a href="https://g.co/brain">Google Research, Brain Team</a>.
              </p>
              <p>
                Prior to Google Brain, I spent two years at Microsoft working on vision foundation models. Featured projects include <a href="https://arxiv.org/abs/2111.11432" target="_blank">FLorence</a> and <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Lei_Less_Is_More_ClipBERT_for_Video-and-Language_Learning_via_Sparse_Sampling_CVPR_2021_paper.html" target="_blank">ClipBERT</a>.
                I received my Ph.D. degree from University of Michigan in 2020, under the supervision of <a href="http://web.eecs.umich.edu/~jjcorso/" tppabs="http://web.eecs.umich.edu/~jjcorso/" target="_blank">Dr. Jason J. Corso</a>. I worked on projects including vision-language pre-training (<a href="https://www.microsoft.com/en-us/research/blog/expanding-scene-and-language-understanding-with-large-scale-pre-training-and-a-unified-architecture/" target="_blank">Unified VLP</a>), <a href="http://youcook2.eecs.umich.edu", target="_blank">YouCook2</a>, and one of the first Visual Transformers (<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.pdf", target="_blank">densecap</a>).
                I received my bachelor's degree from Nanjing University in 2015, where I worked on <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7445162" tppabs="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7445162" target="_blank">Multi-Agent RL</a>. I spent summer interns at FAIR, MSR, and Salesforce Research. I am one of the winners of CVPR 2021 <a href="https://cvpr2021.thecvf.com/node/329" target="_blank">Best Student Paper HM</a>.

              </p>
              <p style="text-align:center">
                <a href="mailto:zhouluoweiwest@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/LuoweiZhou-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=M-3cIR0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/luowei_zhou">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/luowei-zhou-6548aa9a/">Linkedin</a> &nbsp/&nbsp
                <a href="https://github.com/LuoweiZhou">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/LuoweiZhou.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/LuoweiZhou.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <li style="color:black;"> [09/2022] 3 papers accepted at NeurIPS'22. </li>
                <li style="color:black;"> [07/2022] 2 papers accepted at ECCV'22. </li>
                <li style="color:black;"> [06/2022] Florence & Visual Clues are covered by <a href="https://www.economist.com/interactive/briefing/2022/06/11/huge-foundation-models-are-turbo-charging-ai-progress" target="_blank">The Economist</a>! Check out <a href="https://www.youtube.com/watch?v=ZF_48yuBo9M" target="_blank">XD's keynote</a> at CVPR'22.  </li>
                <li style="color:black;"> [03/2022] 3 papers accepted at CVPR'22. </li>
                <li style="color:black;"> [06/2021] Our <a href="https://github.com/jayleicn/ClipBERT" target="_blank">ClipBERT</a> won the <a href="http://cvpr2021.thecvf.com/node/329" target="_blank">CVPR'21 Best Student Paper HM</a>!  </li>
                <li style="color:black;"> [06/2021] My <a href="https://www.youtube.com/watch?v=19Z6cghHWMc" target="_blank">talk</a> on recent advances in video-and-language pre-training from our CVPR'21 vision+language <a href="https://vqa2vln-tutorial.github.io/" target="_blank">tutorial</a>. </li>
                <li style="color:black;"> [06/2021] Congrats to Team RUC and INRIA on winning our CVPR'21 ActivityNet-Entities challenge. <a href="https://www.youtube.com/watch?v=g4ty-0Qb1H4" target="_blank">video</a> and <a href="https://arxiv.org/abs/2106.06138" target="_blank">report</a>. </li>
                <li style="color:black;"> [06/2021] 1 paper accepted at NeurIPS'21 and 1 paper at ACL'21.
                <li style="color:black;"> [06/2021] We will host Video-And-Language Understanding Evaluation challenge (<a href="https://value-leaderboard.github.io/" target="_blank">VALUE</a>) at <a href="https://sites.google.com/view/iccv21clvl" target="_blank">ICCV'21</a>. </li>
                <li style="color:black;"> [03/2021] 2 papers accepted at CVPR'21. </li>
                <li style="color:black;"> [11/2020] Announcing <a href="https://github.com/LuoweiZhou/YouCook2-Leaderboard">YouCook Leaderboard</a>, a one-stop shop for YouCook2 info & leaderboard.  </li>
                <li style="color:black;"> [10/2020] Recognized as top 10% of high-scoring reviewers at NeurIPS'21. </li>
                <li style="color:black;"> [05/2020] Videos from our CVPR'20 tutorial on <a href="https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research/" target="_blank">Recent Advances in V+L</a> are available! </li>
                <li style="color:black;"> [04/2020] My defense recording "Language-Driven Video Understanding" is on <a href="//youtu.be/x2z5pRotlhI" target="_blank">YouTube</a>. Editing credit: Dan Newman. </li>
                <li style="color:black;"> [09/2019] Work with Prof. Justin Johnson on the first Deep Learning for Computer Vision <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/" target="_blank">course</a> at UMich. <a href="https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r" target="_blank">Recordings</a>. </li>              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Research</heading>
            <p>
              I'm interested in computer vision and its relations to natural language and deep learning, with a focus on learning visual
              representation from multimodal supervision. Problems of interest include multimodal learning (e.g., captioning, grounding, VQA), video understanding,
              unsupervised representation learning, generative models, and Transformers etc.
            </p>
          </td>
        </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/visual_clues.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning</papertitle>
              <br><br>
              Yujia Xie, <strong>Luowei Zhou</strong>, Xiyang Dai, Lu Yuan, Nguyen Bach, Ce Liu, Michael Zeng
              <br><br>
              <em>NeurIPS</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2206.01843.pdf">PDF</a> /
              <a href="https://luoweizhou.github.io/captioning_output.html">Examples</a> /
              <a href="https://www.economist.com/interactive/briefing/2022/06/11/huge-foundation-models-are-turbo-charging-ai-progress">Covered by The Economist</a>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vidil.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners</papertitle>
              <br><br>
              Zhenhailong Wang, Manling Li, Ruochen Xu, <strong>Luowei Zhou</strong>, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji
              <br><br>
              <em>NeruIPS</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2205.10747.pdf">PDF</a> /
              <a href="https://github.com/MikeWangWZHL/VidIL">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/omnivl.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>OmniVL: One Foundation Model for Image-Language and Video-Language Tasks</papertitle>
              <br><br>
              Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, <strong>Luowei Zhou</strong>, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, Lu Yuan
              <br><br>
              <em>NeurIPS</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2209.07526.pdf">PDF</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/msclip.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training</papertitle>
              <br><br>
              Haoxuan You*, <strong>Luowei Zhou*</strong>, Bin Xiao*, Noel Codella*, Yu Cheng, Ruochen Xu, Shih-Fu Chang, Lu Yuan
              <br><br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2207.12661.pdf">PDF</a> /
              <a href="https://github.com/Hxyou/MSCLIP">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dna.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>DnA: Improving Few-shot Transfer Learning with Low-Rank Decomposition and Alignment</papertitle>
              <br><br>
              Ziyu Jiang, Tianlong Chen, Xuxi Chen, Yu Cheng, <strong>Luowei Zhou</strong>, Lu Yuan, Ahmed Awadallah, Zhangyang Wang
              <br><br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800229.pdf">PDF</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bevt.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>BEVT: BERT Pretraining of Video Transformers</papertitle>
              <br><br>
              Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, <strong>Luowei Zhou</strong>, Lu Yuan
              <br><br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_BEVT_BERT_Pretraining_of_Video_Transformers_CVPR_2022_paper.pdf">PDF</a> /
              <a href="https://github.com/xyzforever/BEVT">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/clip_event.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>CLIP-Event: Connecting Text and Images With Event Structures</papertitle>
              <br><br>
              Manling Li, Ruochen Xu, Shuohang Wang, <strong>Luowei Zhou</strong>, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, Shih-Fu Chang
              <br><br>
              <em>CVPR</em>, 2022 <strong>(Oral)</strong>
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf">PDF</a> /
              <a href="https://github.com/limanling/clip-event">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/regionclip.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>RegionCLIP: Region-Based Language-Image Pretraining</papertitle>
              <br><br>
              Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, <strong>Luowei Zhou</strong>, Xiyang Dai, Lu Yuan, Yin Li, Jianfeng Gao
              <br><br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.pdf">PDF</a> /
              <a href="https://github.com/microsoft/RegionCLIP">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hand_tracking.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Temporally Guided Articulated Hand Pose Tracking in Surgical Videos</papertitle>
              <br><br>
              Nathan Louis, <strong>Luowei Zhou</strong>, Steven J Yule, Roger D Dias, Milisa Manojlovich, Francis D Pagani, Donald S Likosky, Jason J. Corso
              <br><br>
              <em>IJCARS</em>, 2022
              <br>
              <a href="https://link.springer.com/content/pdf/10.1007/s11548-022-02761-6.pdf">PDF</a> /
              <a href="https://github.com/MichiganCOG/Surgical_Hands_RELEASE">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/florence.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Florence: A New Foundation Model for Computer Vision</papertitle>
              <br><br>
              Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, <strong>Luowei Zhou</strong>, Pengchuan Zhang
              <br><br>
              <em>arXiv</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2111.11432.pdf">PDF</a> /
              <a href="https://www.microsoft.com/en-us/research/blog/azure-ai-milestone-new-foundation-model-florence-v1-0-pushing-vision-and-vision-language-state-of-the-art/">Azure Blog</a> /
              <a href="https://www.youtube.com/watch?v=ZF_48yuBo9M">XD's CVPR'22 Keynote</a> /
              <a href="https://medium.com/syncedreview/microsofts-florence-general-purpose-foundation-model-achieves-sota-results-on-dozens-of-cv-605eb48f59f2">Synced</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/clipbert.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Less Is More: ClipBERT for Video-and-Language Learning via Sparse Sampling</papertitle>
              <br><br>
              Jie Lei, Linjie Li, <strong>Luowei Zhou</strong>, Zhe Gan, Tamara L Berg, Mohit Bansal, Jingjing Liu
              <br>
              <p style="color:red;"><strong>Best Student Paper Honorable Mention award</strong></p>
              <em>CVPR</em>, 2021 <strong>(Oral)</strong>
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lei_Less_Is_More_ClipBERT_for_Video-and-Language_Learning_via_Sparse_Sampling_CVPR_2021_paper.pdf">PDF</a> /
              <a href="https://github.com/jayleicn/ClipBERT">Code</a>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/uc2.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>UC2: Universal Cross-Lingual Cross-Modal Vision-and-Language Pre-Training</papertitle>
              <br><br>
              Mingyang Zhou, <strong>Luowei Zhou</strong>, Shuohang Wang, Yu Cheng, Linjie Li, Zhou Yu, Jingjing Liu
              <br><br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_UC2_Universal_Cross-Lingual_Cross-Modal_Vision-and-Language_Pre-Training_CVPR_2021_paper.pdf">PDF</a> /
              <a href="https://github.com/zmykevin/UC2">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/value.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation</papertitle>
              <br><br>
              Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, <strong>Luowei Zhou</strong>, Xin Eric Wang, William Yang Wang, Tamara Lee Berg, Mohit Bansal, Jingjing Liu, Lijuan Wang, Zicheng Liu
              <br><br>
              <em>NeurIPS</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2106.04632.pdf">PDF</a> /
              <a href="https://value-benchmark.github.io/">Benchmark</a> /
              <a href="https://sites.google.com/view/iccv21clvl/home">ICCV'21 Challenge</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/clusterformer.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Cluster-Former: Clustering-based Sparse Transformer for Question Answering</papertitle>
              <br><br>
              Shuohang Wang, <strong>Luowei Zhou</strong>, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, Jingjing Liu
              <br><br>
              <em>ACL (Findings)</em>, 2021
              <br>
              <a href="https://aclanthology.org/2021.findings-acl.346.pdf">PDF</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/graduation.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Language-Driven Video Understanding</papertitle>
              <br><br>
              <strong>Luowei Zhou</strong>
              <br><br>
              <em>Dissertation</em>, 2020
              <br>
              <a href="https://deepblue.lib.umich.edu/handle/2027.42/155174">PDF</a> /
              <a href="//youtu.be/x2z5pRotlhI">Defense Recording</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vlp.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Unified Vision-Language Pre-Training for Image Captioning and VQA</papertitle>
              <br><br>
              <strong>Luowei Zhou</strong>, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, Jianfeng Gao
              <br><br>
              <em>AAAI</em>, 2020 <strong>(Spotlight)</strong>
              <br>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/7005">PDF</a> /
              <a href="https://github.com/LuoweiZhou/VLP">Code</a> /
              <a href="https://www.microsoft.com/en-us/research/blog/expanding-scene-and-language-understanding-with-large-scale-pre-training-and-a-unified-architecture/">MSR Blog</a> /
              <a href="https://venturebeat.com/ai/microsofts-ai-learns-to-answer-questions-about-scenes-from-image-text-pairs/">VentureBeat</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gvd.png" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Grounded Video Description</papertitle>
              <br><br>
              <strong>Luowei Zhou</strong>, Yannis Kalantidis, Xinlei Chen, Jason J. Corso, Marcus Rohrbach
              <br><br>
              <em>CVPR</em>, 2019 <strong>(Oral)</strong>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf">PDF</a> /
              <a href="https://github.com/facebookresearch/grounded-video-description">Code</a> /
              <a href="https://github.com/facebookresearch/ActivityNet-Entities">ActivityNet-Entities dataset</a> /
              <a href="http://activity-net.org/challenges/2020/tasks/guest_anet_eol.html">CVPR'20 Challenge</a> /
              <a href="https://www.youtube.com/watch?v=g4ty-0Qb1H4">CVPR'21 Challenge</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dynamic_graphs.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Dynamic Graph Modules for Modeling Object-Object Interactions in Activity Recognition</papertitle>
              <br><br>
              Hao Huang, <strong>Luowei Zhou</strong>, Wei Zhang, Jason J. Corso, Chenliang Xu
              <br><br>
              <em>BMVC</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1812.05637.pdf">PDF</a>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/densecap.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>End-to-End Dense Video Captioning With Masked Transformer</papertitle>
              <br><br>
              <strong>Luowei Zhou*</strong>, Yingbo Zhou*, Jason J. Corso, Richard Socher, Caiming Xiong
              <br><br>
              <em>CVPR</em>, 2018 <strong>(Spotlight)</strong>
              <br>
              <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.pdf">PDF</a> /
              <a href="https://github.com/salesforce/densecap">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/youcook2.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Towards Automatic Learning of Procedures from Web Instructional Videos</papertitle>
              <br><br>
              <strong>Luowei Zhou</strong>, Chenliang Xu, Jason J. Corso
              <br><br>
              <em>AAAI</em>, 2018 <strong>(Oral)</strong>
              <br>
              <a href="https://arxiv.org/pdf/1703.09788.pdf">PDF</a> /
              <a href="https://github.com/LuoweiZhou/ProcNets-YouCook2">Code</a> /
              <a href="http://youcook2.eecs.umich.edu/">YouCook2 dataset</a> /
              <a href="https://github.com/LuoweiZhou/YouCook2-Leaderboard">Leaderboard</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/object_grounding.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Weakly-Supervised Video Object Grounding from Text by Loss Weighting and Object Interaction</papertitle>
              <br><br>
              <strong>Luowei Zhou</strong>, Nathan Louis, Jason J. Corso
              <br><br>
              <em>BMVC</em>, 2018
              <br>
              <a href="https://arxiv.org/pdf/1805.02834.pdf">PDF</a> /
              <a href="https://github.com/MichiganCOG/Video-Grounding-from-Text">Code</a> /
              <a href="http://youcook2.eecs.umich.edu/">YouCook2-BB dataset</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/marl.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Multiagent Reinforcement Learning With Sparse Interactions by Negotiation and Knowledge Transfer</papertitle>
              <br><br>
              <strong>Luowei Zhou</strong>, Pei Yang, Chunlin Chen, Yang Gao
              <br>
              <p style="color:red;"><strong>Journal Impact Factor: 19.12</strong></p>
              <em>IEEE Transactions on Cybernetics</em>, 2017
              <br>
              <a href="https://arxiv.org/pdf/1508.05328.pdf">PDF</a> /
              <a href="https://github.com/LuoweiZhou/Negotiation-based-MARL">Code</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/warehouse.jpg" alt="b3do" width="185" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>A Balanced Heuristic Mechanism for Multirobot Task Allocation of Intelligent Warehouses</papertitle>
              <br><br>
              <strong>Luowei Zhou</strong>, Yuanyuan Shi, Jiangliu Wang, Pei Yang
              <br><br>
              <em>MPE</em>, 2014
              <br>
              <a href="https://downloads.hindawi.com/journals/mpe/2014/380480.pdf">PDF</a>
              <p></p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
              <a href="https://jonbarron.info/">Website template courtesy</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
