<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta http-equiv="Content-Location" content="about.html"><meta name="generator" content="Starfield Technologies; Go Daddy Website Builder v7.0.74">
<meta name="description" content="Welcome to Luowei Zhou's Homepage! Luowei Zhou is a Ph.D. student at University of Michigan. He is doing research in deep learning, computer vision and multi-modal embedding."> 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  
  ga('create', 'UA-58631381-1', 'auto');
  ga('send', 'pageview'); 
</script>

<link rel="icon" type="image/png" href="http://umich.edu/skins/um2013/media/images/touch-icon-iphone.png">
<title>Luowei Zhou's Homepage</title>
<link rel="stylesheet" type="text/css" href="../fonts.googleapis.com/css-family=PT+Sans+Narrow-Oswald.css" tppabs="http://fonts.googleapis.com/css?family=PT+Sans+Narrow|Oswald"><link rel="stylesheet" type="text/css" href="site.css-v=204174960.css" tppabs="http://luoweizhou.net/site.css?v=204174960">

<script> if (typeof ($sf) === "undefined") { $sf = { baseUrl: "//img3.wsimg.com/wst/v7/WSB7_J_20140212_1606_i18n_153", skin: "app", "preload": 0, require: { jquery: "../img3.wsimg.com/wst/v7/WSB7_J_20140212_1606_i18n_153/libs/jquery/jq.js"/*tpa=http://img3.wsimg.com/wst/v7/WSB7_J_20140212_1606_i18n_153/libs/jquery/jq.js*/ } }; } </script>
<script id="duel" src="../img3.wsimg.com/starfield/duel/v2.5.7/duel.js-appid=O3BkA5J1#TzNCa0E1SjF2Mi41Ljdwcm9k" tppabs="http://img3.wsimg.com/starfield/duel/v2.5.7/duel.js?appid=O3BkA5J1#TzNCa0E1SjF2Mi41Ljdwcm9k"></script>
</head>

<body>
<style type="text/css"> 
#wsb-element-101210526{top:53px;left:51px;position:absolute;z-index:15}
#wsb-element-101210526 .txt{width:723px;height:62px}
#wsb-element-101210527{top:115px;left:51px;position:absolute;z-index:16}
#wsb-element-101210527 .wsb-line-element{width:798px;height:21px}
#wsb-element-101210528{top:62px;left:531px;position:absolute;z-index:17}
#wsb-element-101210528 .txt{width:301px;height:58px}
#wsb-element-101210529{top:150px;left:51px;position:absolute;z-index:18}
#wsb-element-101210529{width:781px;height:22px}
#wsb-element-101210534{top:27px;left:429.5px;position:absolute;z-index:33}
#wsb-element-101210534 .wsb-image-inner{}
#wsb-element-101210534 .wsb-image-inner div{width:24px;height:24px;position:relative;overflow:hidden}
#wsb-element-101210534 img{position:absolute}
#wsb-element-101210538{top:100px;left:0px;position:absolute;z-index:39}
#wsb-element-101210538 .txt{width:900px;height:21px}
#wsb-element-101210545{top:0px;left:0px;position:absolute;z-index:13}
#wsb-element-101210545 .wsb-shape{width:900px;height:115px;padding:0px;background:#fff;box-sizing:content-box;-moz-box-sizing:content-box}
#wsb-element-101212083{top:225px;left:50px;position:absolute;z-index:51}
#wsb-element-101212083 .txt{width:488px;height:231px}
#wsb-element-101216289{top:230px;left:600px;position:absolute;z-index:52}
#wsb-element-101216289 .wsb-image-inner{}
#wsb-element-101216289 .wsb-image-inner div{width:200px;height:200px;position:relative;overflow:hidden}
#wsb-element-101216289 img{position:absolute} 
#wsb-element-101212084{top:460px;left:50px;position:absolute;z-index:53}
#wsb-element-101212084 .txt{width:800px;height:100px}
</style>


<div class="wsb-canvas body" style="background-color: #333333; background-position-x: left; background-position-y: top; background-position: left top; background-repeat: repeat; position: fixed; top: 0; bottom: 0; left: 0; right: 0; width: 100%; height: 100%; overflow: hidden;">
<div class="wsb-canvas-page-container" style="position: absolute; top: 0; bottom: 0; left: 0; right: 0; width: 100%; height: 100%; overflow: auto;"><div id="wsb-canvas-template-page" class="wsb-canvas-page page" style="height: 1040px; margin: auto; width: 900px; background-color: #ffffff; position: relative; margin-top: 34px"><div id="wsb-canvas-template-container" style="position: absolute;">
<div id="wsb-element-101210526" class="wsb-element-text" data-type="element">
<div class="txt "><h3><span style="font-size:48px;"><span style="color:#696969;"><span style="font-family:&quot;times new roman&quot;, times, serif;">Luowei Zhou</span></span></span></h3></div></div>
<div id="wsb-element-101210527" class="wsb-element-line" data-type="element"> 
<div class="wsb-line-element" style="width: 798px; height: 21px; width: 798px;border-top: 1px solid #c4c4c4;margin: 10px 0;opacity: 1;filter: alpha(opacity=1);"></div></div>
<div id="wsb-element-101210528" class="wsb-element-text" data-type="element">
<div class="txt "><h3 style="text-align: right;"><span style="font-family:times new roman,times,serif;"><span style="color:#000000;"></span></span><span style="font-size:18px;"><span style="font-family:times new roman,times,serif;">MultiModal AI</span></span></h3><h3 style="text-align: right;"><span style="font-size:18px;"><span style="font-family:times new roman,times,serif;"><span style="color: #000000">Microsoft</span></span></span></h3></div></div>
<div id="wsb-element-101210529" class="wsb-element-navigation" data-type="element"><div style="width: 781px; height: 22px;" class="wsb-nav nav_theme nav-text-left nav-horizontal nav-btn-left wsb-navigation-rendered-top-level-container" id="wsb-nav-101210529">
<style> 
#wsb-nav-101210529.wsb-navigation-rendered-top-level-container ul > li:hover, 
#wsb-nav-101210529.wsb-navigation-rendered-top-level-container ul > li:hover > a, 
#wsb-nav-101210529.wsb-navigation-rendered-top-level-container ul > li.active:hover, 
#wsb-nav-101210529.wsb-navigation-rendered-top-level-container ul > li.active > a:hover, #wsb-nav-101210529.wsb-navigation-rendered-top-level-container ul > li.active .nav-subnav li:hover, #wsb-nav-101210529.wsb-navigation-rendered-top-level-container ul > li.active .nav-subnav li:hover > a {background-color: !important;color: !important;}</style>
<ul class="wsb-navigation-rendered-top-level-menu ">
<li style="width: auto" class="active"><a href="about.html" tppabs="http://luoweizhou.net/about.html" target="" data-title="About" data-pageid="394358" data-url="about.html">About</a></li>
<li style="width: auto"><a href="cv.html" tppabs="http://luoweizhou.net/cv.html" target="" data-title="CV" data-pageid="394359" data-url="cv.html">CV</a></li>
<li style="width: auto"><a href="research.html" tppabs="http://luoweizhou.net/research.html" target="" data-title="Research" data-pageid="158417761" data-url="research.html">Research</a></li><li style="width: auto"><a href="contact.html" tppabs="http://luoweizhou.net/contact.html" target="" data-title="Contact" data-pageid="394360" data-url="contact.html">Contact</a></li>
</ul></div></div>

<div id="wsb-element-101212083" class="wsb-element-text" data-type="element"><div class="txt ">
<p style="text-align: justify;"><span style="color:#333333;">I am currently a Researcher at Microsoft. I received my Ph.D. degree from University of Michigan in 2020, under the supervision of <a href="http://web.eecs.umich.edu/~jjcorso/" tppabs="http://web.eecs.umich.edu/~jjcorso/" target="_blank">Dr. Jason J. Corso</a>. I received my bachelor's degree from Nanjing University in 2015.</span></p>
<br><p style="text-align: justify;"><span style="color:#333333;"></span></p>
<p style="text-align: justify; color: rgb(51, 51, 51);">My research focuses on the intersection of computer vision and natural language processing (or vision+language), such as visual captioning, grounding, and question answering. My work intensively relies on deep learning and machine learning algorithms. My most recent efforts are on automatic video understanding, featured projects include vision-language pre-training (<a href="https://www.microsoft.com/en-us/research/blog/expanding-scene-and-language-understanding-with-large-scale-pre-training-and-a-unified-architecture/" target="_blank">VLP</a>), <a href="http://youcook2.eecs.umich.edu", target="_blank">YouCook2</a>, <a href="https://github.com/facebookresearch/grounded-video-description" target="_blank">grounded video description</a>, and <a href="https://github.com/salesforce/densecap", target="_blank">densecap</a>. Previously, I worked on <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7445162" tppabs="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7445162" target="_blank">Multi-Agent RL</a> at Nanjing University. I have spent summer interns at Facebook AI Research, MSR, and Salesforce Research. </p></div></div>

<div id="wsb-element-101216289" class="wsb-element-image" data-type="element"> <div class="wsb-image-inner "><div class="img_border_shadow"><img src="joe_photo.jpg" tppabs="joe_photo.jpg" alt="" style="width:200px;height:200px;"></div></div></div>

<div id="wsb-element-101212084" class="wsb-element-text" data-type="element"><div class="txt "><br><hr>
<h3 style="color:black;">News</h3>
<ul style="text-align:left;">

<li style="color:black;"> <strong>[10/2020] I am looking for self-motivated interns for Spring 2021 (part-time) to work with me and our <a href="https://multimodalai.azurewebsites.net/publications" target="_blank">MultiModal AI team</a> (20+ papers from top venues including CVPR/ECCV/NeurIPS/ACL/EMNLP in 2020) remotely. Topics include V+L, self-supervised learning, insturctional videos, Transformers for vision. Please email me if you are interested. </strong> </li>
<li style="color:black;"> <strong>[05/2020]</strong> Slides and videos from our CVPR'20 tutorial on <a href="https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research/" target="_blank">Recent Advances in V+L</a> are available now! </li>
<li style="color:black;"> <strong>[05/2020]</strong> Excited to join Microsoft <a href="https://multimodalai.azurewebsites.net/" target="_blank">Dynamics 365 AI Research</a>! Check out our team <a href="https://multimodalai.azurewebsites.net/people/members" target="_blank">members</a> and <a href="https://multimodalai.azurewebsites.net/publications" target="_blank">publications</a>. </li>
<li style="color:black;"> <strong>[04/2020]</strong> My thesis defense titled "Language-Driven Video Understanding" is available on <a href="//youtu.be/x2z5pRotlhI" target="_blank">YouTube</a> now. Thanks to Dan for the editing/captions. </li>
<li style="color:black;"> <strong>[04/2020]</strong> CVPR'20 <a href="http://activity-net.org/challenges/2020/tasks/guest_anet_eol.html" target="_blank">Activity-Entities Object Localization (Grounding) Challenge</a> (a part of the annual <a href="http://activity-net.org/challenges/2020/challenge.html" target="_blank">ActivityNet Challenge</a>) has officially started! Click <a href="https://github.com/facebookresearch/ActivityNet-Entities" target="_blank">here</a> for more details! </li>
<li style="color:black;"> <strong>[04/2020]</strong> YouCook2 text-to-video retrieval task is hosted at the CVPR'20 <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/challenge.html" target="_blank">Video Pentathlon</a> Workshop. Also, check out this awesome <a href="http://howto100m.inria.fr/youcook" target="_blank">demo</a> built by Antoine! </li>
<li style="color:black;"> <strong>[11/2019]</strong> Our VLP work is accepted by AAAI'20 (spotlight)! VLP is featured in <a href='https://www.microsoft.com/en-us/research/blog/expanding-scene-and-language-understanding-with-large-scale-pre-training-and-a-unified-architecture/'>MSR blog</a>, <a href='https://venturebeat.com/2019/10/08/microsofts-ai-learns-to-answer-questions-about-scenes-from-image-text-pairs/'>VentureBeat</a>, and <a href='https://towardsdatascience.com/this-microsoft-neural-network-can-answer-questions-about-scenic-images-with-minimum-training-621374f95851'>TDS</a>. </li>
<li style="color:black;"> [09/2019] We introduce our <a href="https://arxiv.org/pdf/1909.11059.pdf">work</a> on Unified Vision-Language Pre-training (VLP), which achieves SotA on image captioning and VQA (datasets: COCO/VQA2.0/Flickr30k) with a single model architecture. Code available on <a href="https://github.com/LuoweiZhou/VLP">Github</a>. Try it out! </li>
<li style="color:black;"> [12/2019] Upcoming services: program committee member/reviewer for CVPR, ECCV, NeurIPS, AAAI, ACL, EMNLP, ICML, IJCAI, and ACM MM etc.</li>
<li style="color:black;"> [09/2019] I am working with Prof. Justin Johnson on a new <a href="http://web.eecs.umich.edu/~justincj/teaching/eecs498/" target="_blank">class</a> on Deep Learning for Computer Vision at UMich. </li>
<li style="color:black;"> [04/2019] We released the source <a href="https://github.com/facebookresearch/grounded-video-description" target="_blank">code</a> for our CVPR'19 oral <a href="https://arxiv.org/pdf/1812.06587.pdf" target="_blank">paper</a> Grounded Video Description! The evaluation server for our dataset <a href="https://github.com/facebookresearch/ActivityNet-Entities" target="_blank">ANet-Entities</a> is live on <a href="https://competitions.codalab.org/competitions/20537" target="_blank">Codalab</a>!</li>
<li style="color:black;"> [04/2019] Our grounded video description <a href="https://arxiv.org/pdf/1812.06587.pdf" target="_blank">paper</a> is accepted by CVPR'19 (oral). We made the ActivityNet-Entities dataset (158k bboxes on 52k captions) available at <a href="https://github.com/facebookresearch/ActivityNet-Entities" target="_blank">Github</a> including evaluation scripts. Source code is on the way!</li>
<li style="color:black;"> [07/2019] Our paper on <a href="https://arxiv.org/pdf/1812.05637.pdf" target="_blank">Dynamic Graph Modules</a> for Activity Recognition is accepted to BMVC'19.</li>
<li style="color:black;"> [04/2019] We released the source <a href="https://github.com/MichiganCOG/Video-Grounding-from-Text" target="_blank">code</a> for our BMVC'18 work on weakly supervised video object grounding! The new YouCook2-BB dataset is available for <a href="http://youcook2.eecs.umich.edu/download" target="_blank">download</a>. The evaluation server is on <a href="https://competitions.codalab.org/competitions/23510" target="_blank">Codalab</a>!</li>
<li style="color:black;"> [03/2019] Serve as a program committee member/reviewer for CVPR, ICCV, TPAMI, IJCV, AAAI, ACM MM etc.</li>
<li style="color:black;"> [02/2019] We released the source <a href="https://github.com/LuoweiZhou/densecap" target="_blank">code</a> on dense video captioning (CVPR 2018).</li>
<li style="color:black;"> [09/2018] Our weakly-supervised video grounding <a href="https://arxiv.org/pdf/1805.02834.pdf" target="_blank">paper</a> is accepted by BMVC 2018. </li>
<li style="color:black;"> [03/2018] Our dense video captioning <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.pdf" target="_blank">paper</a> is accepted by CVPR 2018 (spotlight).</li>
<li style="color:black;"> [02/2018] I will join <a href="https://research.fb.com/category/facebook-ai-research-fair/" target="_blank">Facebook AI Research (FAIR)</a> for Research Intern in summer 2018.</li>
<li style="color:black;"> [02/2018] I will co-organize CVPR'18 Workshop on Fine-grained Instructional Video Understanding (<a href="http://fiver.eecs.umich.edu/" target="_blank">FIVER</a>). </li>
<li style="color:black;"> [11/2017] Our <a href="https://arxiv.org/abs/1703.09788">paper</a> on <a href="http://youcook2.eecs.umich.edu" target="_blank">YouCook2</a> and procedure segmentation is accepted by AAAI 2018 (oral). </li>
</ul>
</div></div></div></div>

<div id="wsb-canvas-template-footer" class="wsb-canvas-page-footer footer" style="margin: auto; min-height:100px; height: 115px; width: 900px; position: relative;"><div id="wsb-canvas-template-footer-container" class="footer-container" style="position: absolute"><div id="wsb-element-101210534" class="wsb-element-image" ><div class="wsb-image-inner "><div class="img"></div></div></div>
<div id="wsb-element-101210545" class="wsb-element-shape" ><div class="wsb-shape shape_rectangle customStyle "></div></div></div></div>
<div class="view-as-mobile" style="padding:10px;position:relative;text-align:center;display:none;"><a href="#" onclick="return false;">View on Mobile</a></div></div></div>


<script type="text/javascript"> require(['http://luoweizhou.net/jq!starfield/jquery.mod', 'modules/cookiemanager/cookiemanager', 'modules/iebackground/iebackground'], function ($, cookieManager, bg) { if (cookieManager.getCookie("WSB.ForceDesktop")) { $('.view-as-mobile', '.wsb-canvas-page-container').show().find('a').bind('click', function () { cookieManager.eraseCookie("WSB.ForceDesktop"); window.location.reload(true); }); } bg.fixBackground(); }); </script>
</body>
</html>
